{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path='C:/armaan/Machine Learning/Datasets/Real Life Violence Dataset'\n",
    "destination_path='C:/armaan/Machine Learning/Datasets/violence splitted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2000 files [00:08, 247.24 files/s]\n"
     ]
    }
   ],
   "source": [
    "split_ratio=(0.75,0.15,0.1)\n",
    "splitfolders.ratio(source_path,destination_path,seed=42,ratio=split_ratio,group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder='C:/armaan/Machine Learning/Datasets/violence splitted/val'\n",
    "output_folder='C:/armaan/Machine Learning/Datasets/frames/val'\n",
    "categories=['Violence','NonViolence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path,output_folder,frame_rate=1):\n",
    "    os.makedirs(output_folder,exist_ok=True)\n",
    "    video_capture=cv2.VideoCapture(video_path)\n",
    "    success,image=video_capture.read()\n",
    "    count=0\n",
    "    file_name_count=1\n",
    "    # fps = int(video_capture.get(cv2.CAP_PROP_FPS))  # Get the frames per second (FPS) of the video\n",
    "\n",
    "    while success:\n",
    "        if count%5==0:\n",
    "            frame_file_name=os.path.join(output_folder,f\"frame{file_name_count}.jpg\")\n",
    "            cv2.imwrite(frame_file_name,image)\n",
    "            file_name_count+=1\n",
    "        success,image=video_capture.read()\n",
    "        count+=1\n",
    "    video_capture.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(input_folder,output_folder,categories,frame_rate=1):\n",
    "    for category in categories:\n",
    "        category_input_path=os.path.join(input_folder,category)\n",
    "        category_output_path=os.path.join(output_folder,category)\n",
    "        \n",
    "        for video_file in tqdm(os.listdir(category_input_path)):\n",
    "            video_path=os.path.join(category_input_path,video_file)\n",
    "            video_name,_=os.path.splitext(video_file)\n",
    "            video_output_folder = os.path.join(category_output_path, video_name)\n",
    "            get_frames(video_path,video_output_folder,frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:55<00:00,  2.71it/s]\n",
      "100%|██████████| 150/150 [00:44<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "process_videos(input_folder,output_folder,categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Armaan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\u001b[1m14536120/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "mobile_net_model = tf.keras.applications.MobileNetV2()\n",
    "mobile_net_model=Model(\n",
    "    inputs=mobile_net_model.inputs,outputs=mobile_net_model.layers[-2].output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in mobile_net_model.layers:\n",
    "    layers.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(frame_path,model):\n",
    "    img=image.load_img(frame_path,target_size=(224,224))\n",
    "    img_array=image.img_to_array(img)\n",
    "    img_array=np.expand_dims(img_array,axis=0)\n",
    "    img_array=preprocess_input(img_array)\n",
    "    features=model.predict(img_array,verbose=False)\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15216883 0.33164126 0.21129113 ... 1.4389979  0.         0.07342909]\n",
      "1280\n"
     ]
    }
   ],
   "source": [
    "#testing the mobilenet model \n",
    "temp=extract_features('C:/armaan/Machine Learning/Datasets/frames/train/NonViolence/NV_4/frame1.jpg',mobile_net_model)\n",
    "print(temp)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frames(frames_folder,model):\n",
    "    labels=[]\n",
    "    features=[]\n",
    "    for category in os.listdir(frames_folder):\n",
    "        category_path=os.path.join(frames_folder,category)\n",
    "        for video_folder in tqdm(os.listdir(category_path)):\n",
    "            video_path=os.path.join(category_path,video_folder)\n",
    "            video_features=[]\n",
    "            for frame in sorted(os.listdir(video_path)):\n",
    "                frame_path=os.path.join(video_path,frame)\n",
    "                curr_features=extract_features(frame_path,model)\n",
    "                video_features.append(curr_features)\n",
    "            features.append(video_features)\n",
    "            labels.append(category)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [20:22<00:00,  1.63s/it]\n",
      "100%|██████████| 750/750 [29:16<00:00,  2.34s/it]  \n"
     ]
    }
   ],
   "source": [
    "train_features,train_labels=process_frames('C:/armaan/Machine Learning/Datasets/frames/train',mobile_net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_features1.pkl', 'wb') as f:\n",
    "    pickle.dump(train_features, f)\n",
    "    \n",
    "with open('train_labels1.pkl', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:52<00:00,  2.75s/it]\n",
      "100%|██████████| 150/150 [10:37<00:00,  4.25s/it] \n"
     ]
    }
   ],
   "source": [
    "val_features,val_labels=process_frames('C:/armaan/Machine Learning/Datasets/frames/val',mobile_net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_features1.pkl', 'wb') as f:\n",
    "    pickle.dump(val_features, f)\n",
    "    \n",
    "with open('val_labels1.pkl', 'wb') as f:\n",
    "    pickle.dump(val_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:15<00:00,  2.55s/it]\n",
      "100%|██████████| 100/100 [05:15<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features,test_labels=process_frames('C:/armaan/Machine Learning/Datasets/frames/test',mobile_net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_features1.pkl', 'wb') as f:\n",
    "    pickle.dump(test_features, f)\n",
    "    \n",
    "with open('test_labels1.pkl', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
